{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e092e441",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59986531",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef663c1c",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics. It describes the probability of an event based on prior knowledge of conditions related to the event. Essentially, it allows us to update the probability estimate for an event as new evidence or information becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28244948",
   "metadata": {},
   "source": [
    "## Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9eb6d",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42e506",
   "metadata": {},
   "source": [
    "The theorem is mathematically expressed as:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "    𝑃(𝐴∣𝐵) is the probability of event 𝐴 occurring given that 𝐵 is true.\n",
    "\n",
    "    𝑃(𝐵∣𝐴) is the probability of event 𝐵 occurring given that 𝐴 is true.\n",
    "\n",
    "    𝑃(𝐴) is the probability of event 𝐴 occurring on its own.\n",
    "\n",
    "    𝑃(𝐵) is the probability of event 𝐵 occurring on its own.\n",
    "\n",
    "Bayes' theorem is widely used in various fields such as medicine, finance, machine learning, and more, for making informed decisions and predictions based on evolving data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8271a",
   "metadata": {},
   "source": [
    "## Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b41342",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c2bae",
   "metadata": {},
   "source": [
    "Bayes' theorem is incredibly versatile and is used in a variety of practical applications across different fields. Here are some key examples:\n",
    "\n",
    "1. Medical Diagnosis\n",
    "\n",
    "Purpose: Updating the probability of a disease based on test results. Example: If a patient tests positive for a rare disease, Bayes' theorem can help determine the actual probability that the patient has the disease, considering the accuracy of the test and the prevalence of the disease.\n",
    "\n",
    "2. Spam Filtering\n",
    "\n",
    "Purpose: Classifying emails as spam or not spam. Example: Email services use Bayes' theorem to calculate the probability that an incoming email is spam based on the presence of certain words or phrases.\n",
    "\n",
    "3. Machine Learning\n",
    "\n",
    "Purpose: Improving the accuracy of predictive models. Example: In classification algorithms, such as Naive Bayes classifiers, Bayes' theorem is used to predict the class of a given data point based on feature values.\n",
    "\n",
    "4. Decision Making\n",
    "\n",
    "Purpose: Making informed decisions under uncertainty. Example: In finance, Bayes' theorem can be used to update the likelihood of a market event occurring, such as a stock price increase, based on new data like earnings reports or economic indicators.\n",
    "\n",
    "5. Risk Assessment\n",
    "\n",
    "Purpose: Evaluating risks in various domains. Example: In insurance, Bayes' theorem helps in updating the probability of claims based on new information about policyholders.\n",
    "\n",
    "6. Forensic Science\n",
    "\n",
    "Purpose: Interpreting evidence in legal cases. Example: It can be used to update the probability of a suspect's guilt based on DNA evidence or witness testimony.\n",
    "\n",
    "7. Natural Language Processing\n",
    "\n",
    "Purpose: Enhancing language models and applications. Example: Bayes' theorem can be applied in language models to predict the probability of a word sequence or in sentiment analysis to determine the sentiment of a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c11c6",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731ed43",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516916a5",
   "metadata": {},
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts in probability theory.\n",
    "\n",
    "**Conditional Probability**\n",
    "\n",
    "Conditional probability, denoted as 𝑃(𝐴∣𝐵), represents the probability of event 𝐴 occurring given that event 𝐵 has already occurred. It is mathematically defined as:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "    𝑃(𝐴∩𝐵) is the probability of both events 𝐴 and 𝐵 occurring together.\n",
    "\n",
    "    𝑃(𝐵) is the probability of event 𝐵 occurring.\n",
    "    \n",
    "**Bayes' Theorem**\n",
    "\n",
    "Bayes' theorem provides a way to update our knowledge of the probability of an event based on new evidence. The theorem relates the conditional probabilities of two events in the following way:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "**Relationship**\n",
    "The relationship between Bayes' theorem and conditional probability can be understood as follows:\n",
    "\n",
    "    Conditional Probability Foundation: Bayes' theorem is fundamentally built upon the concept of conditional probability. It essentially re-expresses the conditional probability of 𝐴 given 𝐵 in terms of the conditional probability of 𝐵 given 𝐴, along with the individual probabilities of 𝐴 and 𝐵.\n",
    "\n",
    "    Updating Beliefs: Bayes' theorem allows for the updating of the probability 𝑃(𝐴∣𝐵) when new evidence 𝐵 is available. This is achieved by combining the prior probability 𝑃(𝐴) (initial belief) with the likelihood 𝑃(𝐵∣𝐴) (how likely the evidence is, given 𝐴).\n",
    "\n",
    "    Reversing Conditions: One of the key aspects of Bayes' theorem is its ability to reverse the conditions. While conditional probability 𝑃(𝐴∣𝐵) tells us the probability of 𝐴 given 𝐵, Bayes' theorem helps us determine 𝑃(𝐴∣𝐵) using 𝑃(𝐵∣𝐴), which might be easier to assess.\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "In essence, conditional probability answers the question: \"Given that 𝐵 has occurred, what is the probability that 𝐴 will occur?\"\n",
    "\n",
    "Bayes' theorem takes this a step further by answering: \"Given that we know 𝑃(𝐵∣𝐴) and the overall probabilities of 𝐴 and 𝐵, what is the updated probability of 𝐴 given the new evidence 𝐵?\"\n",
    "\n",
    "Both concepts are crucial in the field of probability and are widely used in statistical inference, decision making, and various applications where updating beliefs based on new information is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79f869",
   "metadata": {},
   "source": [
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680534e",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52c9bb",
   "metadata": {},
   "source": [
    "Choosing the right type of Naive Bayes classifier for a given problem depends on the nature of your data and the assumptions you can make about the features. Here are the main types of Naive Bayes classifiers and guidelines on when to use each:\n",
    "\n",
    "**1. Gaussian Naive Bayes**\n",
    "\n",
    "Use When: Your features are continuous and follow a normal distribution.\n",
    "\n",
    "Example: Predicting house prices based on continuous features like size, number of rooms, and age of the house.\n",
    "\n",
    "Advantages: Works well with continuous data and is relatively simple to implement.\n",
    "\n",
    "**2. Multinomial Naive Bayes**\n",
    "\n",
    "    Use When: Your features are discrete and represent counts or frequencies.\n",
    "\n",
    "    Example: Text classification problems such as spam detection, where the features are word counts or term frequencies.\n",
    "\n",
    "    Advantages: Specifically designed for count data, making it suitable for text data and document classification.\n",
    "\n",
    "**3. Bernoulli Naive Bayes**\n",
    "\n",
    "    Use When: Our features are binary (0/1 values).\n",
    "\n",
    "    Example: Document classification with binary word occurrence features (word present or not present in a document).\n",
    "\n",
    "    Advantages: Efficient for binary/boolean data, especially when features are sparse.\n",
    "\n",
    "**Steps to Choose the Right Classifier:**\n",
    "\n",
    "    Understand the Data: Analyze the data to determine the type of features (continuous, count-based, or binary).\n",
    "\n",
    "    Assumption Checking: Verify if the data meets the assumptions required by the specific Naive Bayes classifier. For example, check if continuous features approximately follow a normal distribution for Gaussian Naive Bayes.\n",
    "\n",
    "    Experimentation: It’s often useful to try multiple classifiers and evaluate their performance using cross-validation to see which one works best for your specific problem.\n",
    "\n",
    "    Domain Knowledge: Use domain knowledge to guide your choice. Certain types of data and problems are naturally suited to specific classifiers.\n",
    "\n",
    "**Examples in Practice:**\n",
    "\n",
    "    Spam Detection: Typically uses Multinomial Naive Bayes because email content can be represented as word counts or term frequencies.\n",
    "\n",
    "    Medical Diagnosis: Gaussian Naive Bayes might be used if the features are continuous measurements, such as blood pressure or cholesterol levels.\n",
    "\n",
    "    Image Recognition: Bernoulli Naive Bayes can be used if the features are binary, such as pixel intensity (on/off).\n",
    "\n",
    "By considering these factors, you can select the most appropriate Naive Bayes classifier for your problem and achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81b339",
   "metadata": {},
   "source": [
    "## Q6. Assignment: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06316f7",
   "metadata": {},
   "source": [
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc955801",
   "metadata": {},
   "source": [
    "## Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ac19b",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4bbc5",
   "metadata": {},
   "source": [
    "Let's use Naive Bayes to classify the new instance with features 𝑋1=3 and 𝑋2=4 based on the provided dataset. Here are the steps:\n",
    "\n",
    "**Calculate the Prior Probabilities 𝑃(𝐴) and 𝑃(𝐵):**\n",
    "Since the prior probabilities are equal, we have:\n",
    "\n",
    "$$𝑃(𝐴)=𝑃(𝐵)=0.5$$\n",
    "\n",
    "**Calculate the Likelihoods 𝑃(𝑋1=3∣𝐴), 𝑃(𝑋2=4∣𝐴), 𝑃(𝑋1=3∣𝐵), and 𝑃(𝑋2=4∣𝐵):** From the frequency table:\n",
    "\n",
    "$$P(X_1 = 3 | A) = \\frac{4}{10}$$      \n",
    "\n",
    "$$P(X_2 = 4 | A) = \\frac{3}{13}$$\n",
    "\n",
    "$$P(X_1 = 3 | B) = \\frac{1}{5}$$       \n",
    "\n",
    "$$P(X_2 = 4 | B) = \\frac{3}{11}$$\n",
    "\n",
    "**Calculate the Posterior Probabilities:**\n",
    "For class 𝐴:\n",
    "\n",
    "$$P(A | X_1 = 3, X_2 = 4) \\propto P(X_1 = 3 | A) \\cdot P(X_2 = 4 | A) \\cdot P(A)$$\n",
    "\n",
    "$$P(A | X_1 = 3, X_2 = 4) \\propto \\frac{4}{10} \\cdot \\frac{3}{13} \\cdot 0.5$$\n",
    "\n",
    "$$P(A | X_1 = 3, X_2 = 4) \\propto \\frac{12}{130} \\cdot 0.5$$\n",
    "\n",
    "$$P(A | X_1 = 3, X_2 = 4) \\propto \\frac{6}{65}$$\n",
    "\n",
    "For class 𝐵:\n",
    "\n",
    "$$P(B | X_1 = 3, X_2 = 4) \\propto P(X_1 = 3 | B) \\cdot P(X_2 = 4 | B) \\cdot P(B)$$\n",
    "\n",
    "$$P(B | X_1 = 3, X_2 = 4) \\propto \\frac{1}{5} \\cdot \\frac{3}{11} \\cdot 0.5$$\n",
    "\n",
    "$$P(B | X_1 = 3, X_2 = 4) \\propto \\frac{3}{55} \\cdot 0.5$$\n",
    "\n",
    "$$P(B | X_1 = 3, X_2 = 4) \\propto \\frac{3}{110}$$\n",
    "\n",
    "**Comparison**\n",
    "\n",
    "$$\\frac{6}{65} \\text{ vs } \\frac{3}{110}$$\n",
    "\n",
    "**Converting to a common denominator for comparison:**\n",
    "\n",
    "$$\\frac{6 \\times 110}{65 \\times 110} = \\frac{660}{7150}$$\n",
    "\n",
    "$$\\frac{3 \\times 65}{110 \\times 65} = \\frac{195}{7150}$$\n",
    "\n",
    "Since \n",
    "\n",
    "$$\\frac{660}{7150} \\gt \\frac{195}{7150}$$, the posterior probability is higher for class 𝐴."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
